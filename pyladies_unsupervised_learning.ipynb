{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pyladies Berlin Meetup 05.12.17**\n",
    "\n",
    "In this notebook we will apply the unsupervised machine learning methods discussed in the talk on both toy and real world examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will firstly import the required modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** One-Class SVM **\n",
    "\n",
    "We will explore utilising a OC-SVM on a toy data set, and the continue to use it for a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate a 2-D Swiss Roll for testing a OC-SVM. \n",
    "The algorithm is from *S. Marsland, “Machine Learning: An Algorithmic Perspective”, Chapter 10, 2009*. We could have alternatively just taken 2 dimensions from sklearn.data.make_swiss_roll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nSamples = 7000\n",
    "nFeatures = 2\n",
    "nLayers = 5\n",
    "radius = 5\n",
    "nLayers = 5\n",
    "radius = 5\n",
    "rand = np.random.RandomState(1711)\n",
    "\n",
    "t = rand.uniform(low=0, high=1, size=nSamples)\n",
    "toyData1 = np.zeros((nSamples, nFeatures))\n",
    "\n",
    "maxRot = nLayers * 2 * np.pi\n",
    "toyData1[:, 0] = radius * t * np.cos(t * maxRot) + 0.1*np.random.normal(0, 1, nSamples)\n",
    "toyData1[:, 1] = radius *  t * np.sin(t * maxRot)+ 0.1*np.random.normal(0, 1, nSamples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use matplotlib to scatter this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Plot the toy data scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we will create a one-class SVM and train it on the toy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Use sklearn's One Class SVM class with a nu=0.15 and gamma=7 and train it on the toy data set. Set the random_state to 1711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we will plot the data set, the decision surface and the contour plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-10, 10, 500), np.linspace(-10, 10, 500))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12), dpi= 80)\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='orange')\n",
    "plt.scatter(toyData1[:,0], toyData1[:,1], c='white')\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using a One-Class SVM to detect credit card fraud detection ** \n",
    "\n",
    "We will now use a real life example of detecting credit card fraud based on the data set provided by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: Use Pandas to load the credit card transaction data from creditCardTransactions.csv into dfCreditCardData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders. For anonomity reasons the authors transformed the majority of the data using PCA and provided the first 28 Principal Components. The only numerical values they left was the amount of the transaction. The other value is the time. Let us still observe the value ranges uses describe().\n",
    "\n",
    "The dataset is provided by a research collaboration of the Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles). It is under the ODbL 1.0 license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfCreditCardData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us observe how many transactions are normal and how many are fraudulent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfCreditCardData['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are observing the transactions independently we can drop the time feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: Drop the Time column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to divide our data points from their labels. We will then apply a MinMaxScaler and then proceed to apply PCA on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For conssistency of the notation we will apply the label -1 to the anomolous cases and 1 to the normal ones.  \n",
    "dfCreditCardData['Class'] = dfCreditCardData['Class'].replace({0: 1, 1:-1})\n",
    "\n",
    "X = dfCreditCardData.ix[:, 0:29]\n",
    "y = dfCreditCardData.ix[:, 29]\n",
    "\n",
    "#TODO: Apply a Min Max Scaler on X\n",
    "\n",
    "\n",
    "#TODO: Apply a regular PCA on X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate our data and label set into a training using train_test_split. Usually, the test set should be smaller than the training set. Due to the convergence rate of the algorithm, and our time constraints of the meetup, set the test_size to 0.75. Random state=1711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO, Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the fraudulent transactions from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Remove the fradulent training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create a OneClassSVM with an rbf kernel using sklearn and fit it on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a OneClassSVM with an rbf kernel using sklearn and fit it on the training data.  Set nu to 0.2. Random state=1711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the OCSVM on the good data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the fitting is done proceed to classify the remaining test samples and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = ocSVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Breast Cancer **\n",
    "\n",
    "In the next example we will use unsupervised machine learning algorithms in order to detect breast cancer. \n",
    "The data set used is the Breast Cancer Wisconsin (Diagnostic) Data Set, provided by the UCI Machine Learning Repository. The features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. We inspect their characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataBreastCancer = sklearn.datasets.load_breast_cancer()\n",
    "dfBreastCancer = pd.DataFrame(dataBreastCancer.data, columns=[dataBreastCancer.feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfBreastCancer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Load the features into an nXBreastCancer and the labels in yBreastCancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Normalize the data using a min max scaler and then apply t-sne to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Scatter the transformed data and color them according to their labes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use K-Means with 2 clusters to predict the classes of the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Scatter the transformed data with the prediction based on k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
